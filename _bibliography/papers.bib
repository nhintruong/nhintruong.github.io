---
---

@inproceedings{truong2024crossinglinguistic,
  abbr      = {NAACL},
  title     = {Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models},
  author    = {Truong, Nhi N. and Vo, Hien and Tran, Nhat and Nguyen, Duc and Nguyen, Viet Anh and Quan, Tho},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
  year      = {2024},
  month     = {March},
  html      = {https://aclanthology.org/2024.findings-naacl.182/},
  pdf       = {https://drive.google.com/file/d/1t-rn-MR9B1NkxwhPA3NcUXe5TRSBAAWi/view?usp=sharing},
  abstract  = {Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretrained on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights underscore the significance of meticulous fine-tuning with high-quality datasets in enhancing LLM performance.},
  selected  = {true},
  additional_info = {. Featured in <a href="https://www.nytimes.com/2024/07/26/technology/ai-language-gap.html">The New York Times</a> and <a href="https://hai.stanford.edu/news/improving-equity-and-access-non-english-large-language-models">Stanford HAI</a>.}
}

@inproceedings{truong2024hybridtransformer,
  abbr      = {ICLR WS},
  title     = {Hybrid Transformer and Holt-Winter's Method for Time Series Forecasting},
  author    = {Truong, Nhi N. and Quan, Tho},
  booktitle = {Time Series for Health Workshop, International Conference on Learning Representations (ICLR)},
  year      = {2024},
  month     = {March},
  html      = {https://iclr.cc/virtual/2024/23571},
  pdf       = {https://drive.google.com/file/d/1-l11Mp4lmiwvu6ouw8e9EwIetSmh31aI/view?usp=sharing},
  abstract  = {Time series forecasting is an important research topic in machine learning due to its prevalence in social and scientific applications. Multi-model forecasting paradigm, including model hybridization and model combination, is shown to be more effective than single-model forecasting in the M4 competition. In this study, we hybridize exponential smoothing with transformer architecture to capture both levels and seasonal patterns while exploiting the complex non-linear trend in time series data. We show that our model can capture complex trends and seasonal patterns with moderately improvement in comparison to the state-of-the-arts result from the M4 competition.},
  selected  = {true}
}
